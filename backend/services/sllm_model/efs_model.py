from typing import TypedDict, List, Annotated
from langchain_core.documents import Document
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langgraph.graph import StateGraph
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import torch
import os
from dotenv import load_dotenv

# â”€â”€â”€ í™˜ê²½ ë³€ìˆ˜ ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

# â”€â”€â”€ FAISS DB + ìž„ë² ë”© ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
embedding_model = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large-instruct")
vectorstore = FAISS.load_local("card_QA_faiss_db", embedding_model, allow_dangerous_deserialization=True)

# â”€â”€â”€ EFS ëª¨ë¸ ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
efs_model_path = "/mnt/efs/kanana_model"

# â”€â”€â”€ ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ ì •ì˜ (1íšŒ ë¡œë”© í›„ ìºì‹œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_tokenizer = None
_model = None
_generation_config = None

def get_midm_model():
    global _tokenizer, _model, _generation_config

    if _tokenizer is None or _model is None or _generation_config is None:
        print("ðŸ”„ EFSì—ì„œ ëª¨ë¸ ë¡œë”© ì¤‘...")
        _tokenizer = AutoTokenizer.from_pretrained(efs_model_path)
        _model = AutoModelForCausalLM.from_pretrained(
            efs_model_path,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        _generation_config = GenerationConfig.from_pretrained(efs_model_path)
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    return _tokenizer, _model, _generation_config

# â”€â”€â”€ LangGraphìš© ìƒíƒœ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class GraphState(TypedDict):
    question: Annotated[str, "ì§ˆë¬¸"]
    answer: Annotated[str, "ë‹µë³€"]
    score: Annotated[float, "ìœ ì‚¬ë„ ì ìˆ˜"]
    retriever_docs: Annotated[List[Document], "ìœ ì‚¬ë„ ìƒìœ„ë¬¸ì„œ"]

# â”€â”€â”€ LangGraph ë…¸ë“œ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_with_midm(question: str, context: str) -> str:
    tokenizer, model, _ = get_midm_model()

    system_prompt = "..."
    user_prompt = f"ë¬¸ì„œ: {context}\nì§ˆë¬¸: {question}\nìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜."

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    input_ids = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to("cuda")

    output = model.generate(
        input_ids=input_ids,
        max_new_tokens=1024,
        do_sample=True,
        temperature=0.8,
        top_p=0.75,
        top_k=20,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(output[0], skip_special_tokens=True)

def retriever_node(state: GraphState) -> GraphState:
    docs = vectorstore.similarity_search_with_score(state["question"], k=3)
    retrieved_docs = [doc for doc, _ in docs]
    score = docs[0][1] if docs else 0.0
    return {
        "question": state["question"],
        "retriever_docs": retrieved_docs,
        "score": score,
        "answer": ""
    }

def llm_answer_node(state: GraphState) -> GraphState:
    context = "\n---\n".join([doc.page_content for doc in state["retriever_docs"]])
    answer = generate_with_midm(state["question"], context)
    return {
        "question": state["question"],
        "retriever_docs": state["retriever_docs"],
        "score": state["score"],
        "answer": answer
    }

def query_rewrite_node(state: GraphState) -> GraphState:
    # ìƒëžµ ê°€ëŠ¥
    return state

def decide_to_generate(state: GraphState) -> str:
    return "llm_answer" if state["score"] <= 0.5 else "query_rewrite"

# â”€â”€â”€ LangGraph ì›Œí¬í”Œë¡œ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
workflow = StateGraph(GraphState)
workflow.add_node("retriever", retriever_node)
workflow.add_node("llm_answer", llm_answer_node)
workflow.add_node("query_rewrite", query_rewrite_node)
workflow.set_entry_point("retriever")
workflow.add_conditional_edges("retriever", decide_to_generate, {
    "llm_answer": "llm_answer",
    "query_rewrite": "query_rewrite"
})
workflow.add_edge("query_rewrite", "retriever")

# âœ… ìµœì¢… ì»´íŒŒì¼ëœ graph
app_graph = workflow.compile()
